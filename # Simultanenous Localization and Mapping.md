# Simultaneous Localization and Mapping for Field Robots(SLAM)

## The second project inspection

Team members: Li Jiajun, Kong Lingkai, Gu Juanyi

Supervisors: Hao Qi, Li Dachuan

----

## 1. Background

The objective of simaltaneous localization and mapping is to bestow the capability of map-building and state estimation on robots, which is one of the most fundamental prerequisites for an intelligent robot. For field robots, one primary goal is to adjust and optimize SLAM algorithms and make robots operate smoothly on complex field terrains.

During the last two decades, great success has been achieved using SLAM for real-time state estimation and mapping with perceptual sensors such as a lidar or camera. The sensors detect surrounding environments and input the raw data into the processor, which estates the current state and constructs a map using SLAM algorithm.

SLAM is applied in a wide range of fields, including military, householding and automatic driving. Robot vaccums that adopt SLAM to enhance the route planning ability are perfect evidence to demonstrate the popularity of SLAM.

<center><img src="sdjqr.png" width="330"/></center>
<center><font size = 2>Mapping of a robot vaccum</font></center>

## 2. Challenges

Various SLAM algorithms have been developed these days, while one vital flaw still remains. SLAM can achieve great performance on urban roads but perform poorly on field terrains with uneven ground surface and irregular obstacles like leaves and branches. THe detection and mapping accuracy is therefore negatively affected.

## 3. Objective

Our main objective to the end of this semester is to implement lidar-based Lego-Loam and optimized multi-sensor fusion algorithm LVI-SAM in our wheeled robot hardware platform and test its robustness in field environments on campus, and compare the two algorithms' performance. To realize the objective, we will localize the two algorithms and verify their feasibility with our self-recorded dataset. 

## 4. Experiment Platform

* Hardware platform: Self-developed Wheeled Robot, personal computer
* Software platform: Ubuntu, ROS
* Data set: Self-collected mapping data on campus as well as official dataset on github

<center><img src="robot.png" width="250"/></center>
<center><font size = 2>Self-developed Wheeled Robot</font></center>

## 5. Related Work

During the last two decades, great success has been achieved using SLAM for real-time state estimation and mapping in challenging settings with a single perceptual sensor, such as a lidar or camera.

Lidar-based methods can capture the fine details of the environment at a long range, but such methods typically fail when operating in structure-less environments, especially in the wild fields.

Vision-based methods are especially suitable for place recognition and perform well in texture-rich environments, but their performance is sensitive to illumination and rapid motion, and sometimes fail to recognize surfaces with little texture details.

It is evident that the fusion of both modalities would complement the weakness of each other, but in order to perform such fusion algorithm with optimal performance, precise calibration must be guaranteed between sensors. Also, since the approach still relies on lidar, its performance would still be negatively affected by the structure-less environment and therefore accumulate errors that lead to the failure of loop detection because the algorithm fails to recognize an already visited scene due to deviation or sparse point clouds. The picture below shows the clear difference between a failed and a sucessful example.

<center><img src="huihuan.png" width="300"/></center>
<center><font size = 2>Loop Detection</font></center>

## 6. Innovation

In outdoor environmet, the point cloud generated by lidar are often too sparse to demonstrate the characteristic of objects, therefore by introducing other sensors like cameras and IMU and integrating inputs from multiple modalities, we will be able to build maps with clear details. To achieve optimal performance in field terrains, we adopted LVI-SAM, a SLAM approach with the fusion of lidar, camera and IMU.

<center><img src="lvi.png" width="700"/></center>
<center><font size = 2>LVI-SAM</font></center>

However, the weaknesses of such multi-modality approach remain. In outdoor environment, due to the sparseness of point clouds or potential deviation, the attempt to detect loops may fail because the processor may not be able to recognize a scene already visited. To conquer the problem of possible loop test failure, we have come up with the idea of applying Scan Context to LVI-SAM algorithm.

Scan Context is a global descriptor for lidar point cloud, which is especially designed for a sparse and noisy point cloud acquired in outdoor environment. It generates a unique circular 2.5D map for each scene based on the point cloud information, therefore creating a unique label for every frame when generating a map so that the algorithm can estimate the similarity between frames and calibrate the loop detection procedure.

<center><img src="sc.png" width="250"/></center>
<center><font size = 2>Scan Context</font></center>

## 7. Initial Results

We have operated the wheeled robot on campus to verify the hardware capability and familiarize with the control procedure of the wheeled robot. In addition, we have recorded bag files that store the sensors' raw input data shown below. The left one was recorded in the engineering buiding and the right one was on one of the hillsides in SUSTech, representing the structured environment and the structure-less environment.

<center><img src="raw data.png" width="326"/> <img src="raw data2.png" width="350"/></center>
<center><font size = 2>Two recordings of raw data input</font></center>

Then we applied Lego-Loam algorithm to the self-recorded dataset. Lego-Loam is a lidar-based method, which means it relies solely on lidar and its point clouds to generate a map. The corresponding results are shown below. 

<center><img src="lego1.png" width="350"/> <img src="lego2.png" width="350"/></center>
<center><font size = 2>Maps generated by Lego-Loam</font></center>

We observed ghosting in the final point cloud map, which indicates the fact that loop detection of Lego-Loam failed under the experiment circumstance.

<center><img src="ghosting.png" width="300"/></center>
<center><font size = 2>Ghosting</font></center>

Respectively, LVI-SAM is tested to verify its result. However, since the key adjustments for the algorithm to be compatible to our robot are not completed, we ran online official test cases on our PC to evaluate its output. A corresponding screenshot is shown below.

<center><img src="lvi tu.png" width="600"/></center>
<center><font size = 2>Map generated by LVI-SAM</font></center>

The prelimary experiment conducted on LVI-SAM provided a more optimal result with higher resolution as well as higher accuracy. Compared with our local result generated by Lego-loam, the mapping detail of LVI-SAM is significantlly more explicit. For instance, the point cloud of tree branches and leaves are precisely generated as their shapes in reality, while the point cloud generated by Lego-loam is so sparse that we could merely identify the approximate profile of a tree.

More experiments are to be conducted in the following month, covering the field tests of LVI-SAM and comparative tests of LVI-SAM and Lego-Loam to evaluate the feasibility of our optimization.

## 9. Scheduling

* 2021.11.1-2021.12.3: Collect point cloud and visual data from campus environment, modify the LVI-SAM algorithm, and run the algorithm locally on PC to verify the feasibility of the algorithm on our wheeled robot.
* 2021.12.4-2021.1.10: Apply optimized LVI-SAM algorithm on the wheeled robot platform and commence field tests, and compare the performance of Lego-Loam and LVI-SAM to verify the effectiveness of our optimization.

## 10. Member Contribution

* Li Jiajun: manage project progress, assist with model modification, write the experiment report
* Kong Lingkai: operate the software platform and adjust codes, present our project
* Gu Juanyi: in charge of field tests, assist with model modification

## Reference

[1] Debeunne, C., & Vivet, D. (2020). A review of visual-LiDAR fusion based simultaneous localization and mapping. Sensors, 20(7), 2068.

[2] Kim, G., Choi, S., & Kim, A. (2021). Scan Context++: Structural Place Recognition Robust to Rotation and Lateral Variations in Urban Environments. IEEE Transactions on Robotics.
 
[3] Shan, T., Englot, B., Ratti, C., & Rus, D. (2021). LVI-SAM: Tightly-coupled Lidar-Visual-Inertial Odometry via Smoothing and Mapping. arXiv preprint arXiv:2104.10831.

[4] Shan, T., & Englot, B. (2018, October). Lego-loam: Lightweight and ground-optimized lidar odometry and mapping on variable terrain. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (pp. 4758-4765). IEEE.




